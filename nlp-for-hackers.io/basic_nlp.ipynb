{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Katrina Kaif was born in Hong Kong with her mothers surname Turquotte (also spelt Turcotte), on 16 July 1983.[a][6][7][8] According to the actress, her father (Mohammed Kaif) is a British businessman'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Katrina', 'NNP'), ('Kaif', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Hong', 'NNP'), ('Kong', 'NNP'), ('with', 'IN'), ('her', 'PRP$'), ('mothers', 'NNS'), ('surname', 'VBP'), ('Turquotte', 'NNP'), ('(', '('), ('also', 'RB'), ('spelt', 'VBP'), ('Turcotte', 'NNP'), (')', ')'), (',', ','), ('on', 'IN'), ('16', 'CD'), ('July', 'NNP'), ('1983', 'CD'), ('.', '.'), ('[', 'VB'), ('a', 'DT'), (']', 'JJ'), ('[', 'NN'), ('6', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('7', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('8', 'CD'), (']', 'NN'), ('According', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('actress', 'NN'), (',', ','), ('her', 'PRP$'), ('father', 'NN'), ('(', '('), ('Mohammed', 'NNP'), ('Kaif', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('British', 'JJ'), ('businessman', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/f3/37504f07651330ddfdefa631ca5246974a60d0908216539efda842fd080f/gensim-3.5.0-cp36-cp36m-manylinux1_x86_64.whl (23.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 23.5MB 207kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting numpy>=1.11.3 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/94/7049fed8373c52839c8cde619acaf2c9b83082b935e5aa8c0fa27a4a8bcc/numpy-1.15.1-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 536kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting scipy>=0.18.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 31.2MB 473kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/3d/5f3a9a296d0ba8e00e263a8dee76762076b9eb5ddc254ccaa834651c8d65/smart_open-1.6.0.tar.gz\n",
      "Requirement already satisfied: six>=1.5.0 in ./env/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 817kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Collecting requests (from smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.4MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/45/810f786ce144bfd19d9f2f700a8cd4358435559a2b88b2c235f7bb3f29df/boto3-1.8.6-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.8,>=2.5 (from requests->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 7.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting urllib3<1.24,>=1.21.1 (from requests->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/f7/04fee6ac349e915b82171f8e23cee63644d83663b34c539f7a09aed18f9e/certifi-2018.8.24-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 975kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.12.0,>=1.11.6 (from boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/86/73e2825c9f452f6ea7ebbd715aa4621eb1f5d5e51e37cd2019437535909c/botocore-1.11.6-py2.py3-none-any.whl (4.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.6MB 701kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in ./env/lib/python3.6/site-packages (from botocore<1.12.0,>=1.11.6->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Collecting docutils>=0.10 (from botocore<1.12.0,>=1.11.6->boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 465kB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/harrypotter0/.cache/pip/wheels/73/f1/9b/ccf93d4ba073b6f79b1ed9df68ab5ce048d8136d0efcf90b30\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/harrypotter0/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: numpy, scipy, boto, bz2file, idna, urllib3, chardet, certifi, requests, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.8.6 botocore-1.11.6 bz2file-0.98 certifi-2018.8.24 chardet-3.0.4 docutils-0.14 gensim-3.5.0 idna-2.7 jmespath-0.9.3 numpy-1.15.1 requests-2.19.1 s3transfer-0.1.13 scipy-1.1.0 smart-open-1.6.0 urllib3-1.23\n"
     ]
    }
   ],
   "source": [
    "! pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7fe9cf909240>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 2)],\n",
       " [(0, 1),\n",
       "  (9, 1),\n",
       "  (11, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(17, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.ldamodel.LdaModel"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "Lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x7fe9b0318a20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.053*\"driving\" + 0.053*\"sister\" + 0.053*\"my\" + 0.053*\"My\" + 0.053*\"father\" + 0.053*\"around\" + 0.053*\"a\" + 0.053*\"time\" + 0.053*\"spends\" + 0.053*\"lot\"'), (1, '0.089*\"to\" + 0.051*\"My\" + 0.051*\"sister\" + 0.051*\"my\" + 0.051*\"bad\" + 0.051*\"sugar,\" + 0.051*\"have\" + 0.051*\"is\" + 0.051*\"Sugar\" + 0.051*\"but\"'), (2, '0.060*\"driving\" + 0.060*\"increased\" + 0.060*\"and\" + 0.060*\"may\" + 0.060*\"pressure.\" + 0.060*\"suggest\" + 0.060*\"blood\" + 0.060*\"that\" + 0.060*\"stress\" + 0.060*\"cause\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Text', 'data', 'can', 'also'],\n",
       " ['data', 'can', 'also', 'be'],\n",
       " ['can', 'also', 'be', 'quantified'],\n",
       " ['also', 'be', 'quantified', 'directly'],\n",
       " ['be', 'quantified', 'directly', 'into'],\n",
       " ['quantified', 'directly', 'into', 'numbers'],\n",
       " ['directly', 'into', 'numbers', 'using'],\n",
       " ['into', 'numbers', 'using', 'several'],\n",
       " ['numbers', 'using', 'several', 'techniques'],\n",
       " ['using', 'several', 'techniques', 'described'],\n",
       " ['several', 'techniques', 'described', 'in'],\n",
       " ['techniques', 'described', 'in', 'this'],\n",
       " ['described', 'in', 'this', 'section']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## N-Grams as Features\n",
    "def generate_ngrams(data, n):\n",
    "    words = data.split()\n",
    "    output = []\n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "generate_ngrams(\"Text data can also be quantified directly into numbers using several techniques described in this section\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/c8/8db4108aba5e2166cd2ea4eafa1a4b82f89240a1fa85733029cc2358ad1f/scikit_learn-0.19.2-cp36-cp36m-manylinux1_x86_64.whl (4.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.9MB 302kB/s ta 0:00:01    35% |███████████▍                    | 1.8MB 219kB/s eta 0:00:15\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/harrypotter0/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.19.2 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.5844829010200651\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 1)\t0.34520501686496574\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (2, 4)\t0.444514311537431\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "x = obj.fit_transform(corpus)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1143167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harrypotter0/Documents/ai-blogs/nlp-for-hackers.io/env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/harrypotter0/Documents/ai-blogs/nlp-for-hackers.io/env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('data','science'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3873295e-03  2.3033831e-03  1.3621884e-03  4.4016526e-03\n",
      " -6.3599604e-05  3.0996515e-03  7.7004917e-04 -1.6292006e-03\n",
      "  2.1760168e-03 -2.9454005e-03  3.1981990e-03  3.2678484e-03\n",
      "  1.2671098e-03  2.3010809e-03  4.1117459e-03 -2.0466875e-03\n",
      " -2.1940265e-03 -1.4418317e-03  8.9528388e-04 -4.0952838e-03\n",
      "  1.9320569e-04 -1.6987172e-03 -2.1435224e-04  3.1981040e-03\n",
      "  2.0426824e-03 -2.4997089e-03  2.1329471e-03 -1.7786834e-03\n",
      "  3.4545024e-04  3.3003720e-03  6.1934412e-04  2.1971446e-03\n",
      " -8.4886351e-04 -1.9912570e-04 -1.4199649e-03  4.1418639e-04\n",
      "  2.8057976e-03 -4.0234174e-03 -4.5166537e-03  4.0440294e-03\n",
      "  4.4184262e-03 -4.0756091e-03 -2.2929506e-03  3.7139645e-03\n",
      " -1.3245094e-03 -1.8181578e-04 -4.2366139e-03 -1.1302023e-03\n",
      "  1.0200655e-03  1.9971812e-03 -3.9073629e-03  2.8692719e-03\n",
      " -4.3613762e-03 -1.8282566e-04  4.2012399e-03 -1.7488628e-03\n",
      " -2.9631334e-03  1.1684097e-03  2.0194727e-03  4.9751848e-03\n",
      "  2.1415632e-03 -4.6571228e-03 -8.4377819e-04  2.6858402e-03\n",
      "  4.0199533e-03  4.6029058e-03  3.1182810e-03  1.6700455e-03\n",
      " -1.3672427e-03 -4.1344645e-03  3.4950648e-03 -3.2672787e-03\n",
      " -4.9109668e-03  3.1122183e-03  4.6696956e-03 -4.2459760e-03\n",
      " -3.8636036e-03  2.9524020e-03 -2.2725377e-03 -3.2072845e-03\n",
      " -1.9450546e-03 -6.0648774e-04  1.4203571e-03  2.4697639e-03\n",
      " -3.2810208e-03 -3.0398236e-03  1.8538572e-03  4.6849013e-03\n",
      "  3.5667436e-03  4.4502546e-03  9.8254706e-04 -3.3069889e-03\n",
      " -1.1116301e-03 -3.0774206e-03 -3.6939124e-03 -1.6736364e-03\n",
      " -1.2603363e-03 -3.9956258e-03  4.6756424e-04  1.5677300e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harrypotter0/Documents/ai-blogs/nlp-for-hackers.io/env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/18/7f55c8be6d68ddc4036ffda5382ca51e23a1075987f708b9123712091af1/textblob-0.15.1-py2.py3-none-any.whl (631kB)\n",
      "\u001b[K    100% |████████████████████████████████| 634kB 159kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: nltk>=3.1 in ./env/lib/python3.6/site-packages (from textblob) (3.3)\n",
      "Requirement already satisfied, skipping upgrade: six in ./env/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.1\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/harrypotter0/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/harrypotter0/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/harrypotter0/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/harrypotter0/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /home/harrypotter0/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/harrypotter0/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "! python3 -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier as NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NB(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n"
     ]
    }
   ],
   "source": [
    "print(model.classify(\"Their codes are amazing.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(model.accuracy(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Scikit Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "\n",
    "test_data = [] \n",
    "test_labels = [] \n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the feature vectors\n",
    "train_vectors = vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model on test data \n",
    "test_vectors = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform classification with SVM, kernel=linear \n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(train_vectors, train_labels)\n",
    "prediction = model.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Class_A       0.50      0.67      0.57         3\n",
      "    Class_B       0.50      0.33      0.40         3\n",
      "\n",
      "avg / total       0.50      0.50      0.49         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.17.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Fuzzy in ./env/lib/python3.6/site-packages (1.2.2)\r\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-10b40a18353a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuzzy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msoundex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuzzy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoundex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ankit'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "!pip install Fuzzy\n",
    "import fuzzy \n",
    "soundex = fuzzy.Soundex(4) \n",
    "print(soundex('ankit'.decode('utf-8', 'replace'))) ## this is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.629940788348712"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x]* vec2[x] for x in common])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1)*math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator)/denominator\n",
    "    \n",
    "def text_to_vector(text):\n",
    "    words = text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an article on analytics vidhya' \n",
    "text2 = 'article on analytics vidhya is about natural language processing'\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "cosine = get_cosine(vector1, vector2)\n",
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
